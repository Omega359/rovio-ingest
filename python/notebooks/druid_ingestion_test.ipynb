{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2021 Rovio Entertainment Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PRE-REQUISITES:\n",
    "\n",
    "1. Build the package (shaded jar) on command line:\n",
    "\n",
    "        mvn package -DskipTests\n",
    "\n",
    "2. A) Copy the shaded jar to s3:\n",
    "\n",
    "        AWS_PROFILE=smoke\n",
    "        JAR_BUCKET=<REPLACE THIS>\n",
    "        aws s3 --profile $AWS_PROFILE cp \\\n",
    "          target/rovio-ingest-1.0.0_spark_3.0.1-SNAPSHOT.jar \\\n",
    "          s3://$JAR_BUCKET/tmp/juho/druid/jars/rovio-ingest-1.0.0_spark_3.0.1-SNAPSHOT.jar\n",
    "\n",
    "2. B) Copy the plain jar to s3: \n",
    "\n",
    "        aws s3 --profile $AWS_PROFILE cp \\\n",
    "          target/original-rovio-ingest-1.0.0_spark_3.0.1-SNAPSHOT.jar \\\n",
    "          s3://$JAR_BUCKET/tmp/juho/druid/jars/original-rovio-ingest-1.0.0_spark_3.0.1-SNAPSHOT.jar\n",
    "\n",
    "3. Build a zip of the python wrapper:\n",
    "\n",
    "        cd python \\\n",
    "          && zip --exclude='*.pyc' --exclude='*__pycache__*' --exclude='*~' --exclude='.pytest_cache' \\\n",
    "            -FSr ../target/rovio_ingest.zip rovio_ingest ; cd ..\n",
    "\n",
    "4. Copy the zip to s3:\n",
    "\n",
    "        aws s3 --profile $AWS_PROFILE cp \\\n",
    "          target/rovio_ingest.zip \\\n",
    "          s3://$JAR_BUCKET/tmp/juho/druid/python/rovio_ingest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ENV = \"smoke\"\n",
    "PREFIX = \"tmp/juho/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ssm_client = boto3.session.Session(profile_name=ENV).client(service_name=\"ssm\")\n",
    "\n",
    "# secrets can be added at\n",
    "# https://console.aws.amazon.com/systems-manager/parameters/?region=us-east-1\n",
    "def get_param(secret_name: str) -> str:\n",
    "    return ssm_client.get_parameter(Name=\"/dataengineering/\" + secret_name)[\"Parameter\"][\"Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython import get_ipython\n",
    "\n",
    "def set_spark_config(conf_dict):\n",
    "    get_ipython().run_cell_magic('spark', 'config', json.dumps(conf_dict))\n",
    "\n",
    "def create_spark_session_with_host(host):\n",
    "    get_ipython().run_line_magic('spark', 'add -l python -u http://{}:8998'.format(host))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1617264445938_0029</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-10-102.cloud-dev.rovio.com:20888/proxy/application_1617264445938_0029/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-10-164.cloud-dev.rovio.com:8042/node/containerlogs/container_1617264445938_0029_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "packages_bucket = get_param(\"rovio-ingest/packages_bucket\")\n",
    "\n",
    "spark_conf = {\n",
    "  \"conf\": {\n",
    "    \"spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive\": \"true\",\n",
    "    \"spark.sql.hive.caseSensitiveInferenceMode\": \"NEVER_INFER\",\n",
    "    \"spark.pyspark.python\": \"python3\",\n",
    "    \"spark.sql.session.timeZone\": \"UTC\",\n",
    "    \"spark.submit.pyFiles\": f\"s3://{packages_bucket}/{PREFIX}druid/python/rovio_ingest.zip\",\n",
    "  }\n",
    "}\n",
    "\n",
    "if True:\n",
    "  # A) if using the shaded jar, only this jar:\n",
    "  spark_conf[\"conf\"][\"spark.jars\"] = \\\n",
    "    f\"s3://{packages_bucket}/{PREFIX}druid/jars/rovio-ingest-1.0.0_spark_3.0.1-SNAPSHOT.jar\"\n",
    "else:\n",
    "  # NOTE!! The safe bet against all the hacks below is to just build the shaded jar and use it ^\n",
    "\n",
    "  # The conf below used to work with Spark 2, but is probably broken now\n",
    "\n",
    "  # B) any way, if using the plain jar + maven deps:\n",
    "\n",
    "  # required to avoid guava version conflicts\n",
    "  # (with shaded jar there would be no such problem)\n",
    "  spark_conf[\"conf\"][\"spark.executor.userClassPathFirst\"] = \"true\"\n",
    "  spark_conf[\"conf\"][\"spark.driver.userClassPathFirst\"] = \"true\"\n",
    "\n",
    "  # manually specified maven dependencies as in pom.xml to be able to use a plain (not uber) jar\n",
    "  # (these are not going to be needed eventually. instead only: \"com.rovio:rovio-ingest:1.0\")\n",
    "  maven_deps = [\n",
    "    \"org.apache.druid:druid-server:0.13.0-incubating\",\n",
    "    \"org.apache.druid.extensions:druid-s3-extensions:0.13.0-incubating\",\n",
    "    \"org.apache.druid.extensions:mysql-metadata-storage:0.13.0-incubating\",\n",
    "    \"mysql:mysql-connector-java:5.1.38\",\n",
    "  ]\n",
    "  \n",
    "  maven_excludes = [\n",
    "    # UNSAFE! But seems to work as a workaround for now.\n",
    "    # Without this exclusion druid ingestion fails on EMR\n",
    "    #\n",
    "    # The error from Livy Session log is:\n",
    "    # 20/04/13 07:38:14 ERROR SparkContext: Error initializing SparkContext.\n",
    "    #   java.io.FileNotFoundException:\n",
    "    #   File file:/var/lib/livy/.ivy2/jars/io.netty_netty-transport-native-epoll-4.1.29.Final.jar\n",
    "    #   does not exist\n",
    "    #\n",
    "    # On EMR instance there is actually this:\n",
    "    # /var/lib/livy/.ivy2/jars/io.netty_netty-transport-native-epoll-4.1.29.Final-linux-x86_64.jar\n",
    "    #\n",
    "    # -> So there is a Classifier in there (\"linux-x86_64\"), which spark.jars.packages doesn't \n",
    "    # support! Discussed at least in these issues:\n",
    "    #   https://issues.apache.org/jira/browse/SPARK-20075\n",
    "    #   https://issues.apache.org/jira/browse/SPARK-24287\n",
    "    #\n",
    "    # -> So we just exclude it and hope that the druid modules that rovio-ingest depends on doesn't\n",
    "    # actually need to import anything from it. Seems like we were lucky for now \\o/\n",
    "    \"io.netty:netty-transport-native-epoll\",\n",
    "  ]\n",
    "  \n",
    "  # for testing, include the plain rovio-ingest like this:\n",
    "  # not going to be needed eventually when using \"com.rovio:rovio-ingest:1.0\" in spark.jars.packages:\n",
    "  spark_conf[\"conf\"][\"spark.jars\"] = \\\n",
    "    f\"s3://{packages_bucket}/{PREFIX}druid/jars/original-rovio-ingest-1.0.0_spark_3.0.1-SNAPSHOT.jar\" \n",
    "\n",
    "  spark_conf[\"conf\"][\"spark.jars.packages\"] = \",\".join(maven_deps)\n",
    "  spark_conf[\"conf\"][\"spark.jars.excludes\"] = \",\".join(maven_excludes)\n",
    "  # add https://clojars.org/repo/ to avoid \"module not found: org.hyperic#sigar;1.6.5.132\" on EMR\n",
    "  spark_conf[\"conf\"][\"spark.jars.repositories\"] = \"https://clojars.org/repo/\"\n",
    "\n",
    "set_spark_config(spark_conf)\n",
    "create_spark_session_with_host(get_param(\"spark3/shared/host\"))\n",
    "\n",
    "# to debug problems in session creation, see livy session logs at http://{host}:8998/ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "import boto3\n",
    "\n",
    "ssm_client = boto3.session.Session(region_name=\"us-east-1\").client(service_name=\"ssm\")\n",
    "\n",
    "def get_param(secret_name: str) -> str:\n",
    "    return ssm_client.get_parameter(Name=\"/dataengineering/\" + secret_name)[\"Parameter\"][\"Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------------------+\n",
      "|dau|revenue|    app_id|         event_date|\n",
      "+---+-------+----------+-------------------+\n",
      "|  5|   30.0|testclient|2018-10-01 00:00:00|\n",
      "|  2|   15.0|testclient|2018-10-02 00:00:00|\n",
      "+---+-------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f, types as t, SparkSession\n",
    "\n",
    "spark: SparkSession = spark\n",
    "schema = 'dau:BIGINT, revenue:DOUBLE, app_id:STRING, event_date:TIMESTAMP'\n",
    "df = spark.createDataFrame([[5, 30.0, 'testclient', datetime(2018, 10, 1)],\n",
    "                            [2, 15.0, 'testclient', datetime(2018, 10, 2)]],\n",
    "                            schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __PARTITION_NUM__#172]\n",
      "+- RepartitionByExpression [__PARTITION_TIME__#158, __PARTITION_NUM__#172], 200\n",
      "   +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __num_rows__#165, cast((cast((__num_rows__#165 - 1) as double) / cast(5000000 as double)) as int) AS __PARTITION_NUM__#172]\n",
      "      +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __num_rows__#165]\n",
      "         +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __num_rows__#165, __num_rows__#165]\n",
      "            +- Window [row_number() windowspecdefinition(__PARTITION_TIME__#158, __PARTITION_TIME__#158 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS __num_rows__#165], [__PARTITION_TIME__#158], [__PARTITION_TIME__#158 ASC NULLS FIRST]\n",
      "               +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158]\n",
      "                  +- Project [dau#0L, revenue#1, app_id#2, event_date#3, cast((cast(if (isnull(cast((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * cast(1000 as bigint)) as bigint))) null else UDF(knownnotnull(cast((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * cast(1000 as bigint)) as bigint))) as double) / cast(1000 as double)) as timestamp) AS __PARTITION_TIME__#158]\n",
      "                     +- LogicalRDD [dau#0L, revenue#1, app_id#2, event_date#3], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "dau: bigint, revenue: double, app_id: string, event_date: timestamp, __PARTITION_TIME__: timestamp, __PARTITION_NUM__: int\n",
      "Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __PARTITION_NUM__#172]\n",
      "+- RepartitionByExpression [__PARTITION_TIME__#158, __PARTITION_NUM__#172], 200\n",
      "   +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __num_rows__#165, cast((cast((__num_rows__#165 - 1) as double) / cast(5000000 as double)) as int) AS __PARTITION_NUM__#172]\n",
      "      +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __num_rows__#165]\n",
      "         +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, __num_rows__#165, __num_rows__#165]\n",
      "            +- Window [row_number() windowspecdefinition(__PARTITION_TIME__#158, __PARTITION_TIME__#158 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS __num_rows__#165], [__PARTITION_TIME__#158], [__PARTITION_TIME__#158 ASC NULLS FIRST]\n",
      "               +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158]\n",
      "                  +- Project [dau#0L, revenue#1, app_id#2, event_date#3, cast((cast(if (isnull(cast((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * cast(1000 as bigint)) as bigint))) null else UDF(knownnotnull(cast((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * cast(1000 as bigint)) as bigint))) as double) / cast(1000 as double)) as timestamp) AS __PARTITION_TIME__#158]\n",
      "                     +- LogicalRDD [dau#0L, revenue#1, app_id#2, event_date#3], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "RepartitionByExpression [__PARTITION_TIME__#158, __PARTITION_NUM__#172], 200\n",
      "+- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, cast((cast((__num_rows__#165 - 1) as double) / 5000000.0) as int) AS __PARTITION_NUM__#172]\n",
      "   +- Window [row_number() windowspecdefinition(__PARTITION_TIME__#158, __PARTITION_TIME__#158 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS __num_rows__#165], [__PARTITION_TIME__#158], [__PARTITION_TIME__#158 ASC NULLS FIRST]\n",
      "      +- Project [dau#0L, revenue#1, app_id#2, event_date#3, cast((cast(if (isnull((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * 1000))) null else UDF(knownnotnull((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * 1000))) as double) / 1000.0) as timestamp) AS __PARTITION_TIME__#158]\n",
      "         +- LogicalRDD [dau#0L, revenue#1, app_id#2, event_date#3], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange hashpartitioning(__PARTITION_TIME__#158, __PARTITION_NUM__#172, 200), false, [id=#455]\n",
      "   +- Project [dau#0L, revenue#1, app_id#2, event_date#3, __PARTITION_TIME__#158, cast((cast((__num_rows__#165 - 1) as double) / 5000000.0) as int) AS __PARTITION_NUM__#172]\n",
      "      +- Window [row_number() windowspecdefinition(__PARTITION_TIME__#158, __PARTITION_TIME__#158 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS __num_rows__#165], [__PARTITION_TIME__#158], [__PARTITION_TIME__#158 ASC NULLS FIRST]\n",
      "         +- Sort [__PARTITION_TIME__#158 ASC NULLS FIRST, __PARTITION_TIME__#158 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(__PARTITION_TIME__#158, 1000), true, [id=#451]\n",
      "               +- Project [dau#0L, revenue#1, app_id#2, event_date#3, cast((cast(if (isnull((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * 1000))) null else UDF(knownnotnull((unix_timestamp(event_date#3, yyyy-MM-dd HH:mm:ss, Some(UTC)) * 1000))) as double) / 1000.0) as timestamp) AS __PARTITION_TIME__#158]\n",
      "                  +- Scan ExistingRDD[dau#0L,revenue#1,app_id#2,event_date#3]"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "from rovio_ingest import DRUID_SOURCE\n",
    "from rovio_ingest.extensions.dataframe_extension import ConfKeys, add_dataframe_druid_extension\n",
    "\n",
    "# fix df.explain on EMR 6\n",
    "java_import(spark._sc._jvm, \"org.apache.spark.sql.api.python.*\")\n",
    "\n",
    "add_dataframe_druid_extension()\n",
    "\n",
    "df_prepared = df.repartition_by_druid_segment_size('event_date', segment_granularity='DAY')\n",
    "df_prepared.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dau: long (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- event_date: timestamp (nullable = true)\n",
      " |-- __PARTITION_TIME__: timestamp (nullable = true)\n",
      " |-- __PARTITION_NUM__: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "df_prepared.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------------------+-------------------+-----------------+\n",
      "|dau|revenue|    app_id|         event_date| __PARTITION_TIME__|__PARTITION_NUM__|\n",
      "+---+-------+----------+-------------------+-------------------+-----------------+\n",
      "|  2|   15.0|testclient|2018-10-02 00:00:00|2018-10-02 00:00:00|                0|\n",
      "|  5|   30.0|testclient|2018-10-01 00:00:00|2018-10-01 00:00:00|                0|\n",
      "+---+-------+----------+-------------------+-------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "df_prepared.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "DATA_SOURCE_NAME = \"rovio_ingest_test_juho\"\n",
    "\n",
    "df_prepared \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(DRUID_SOURCE) \\\n",
    "    .option(ConfKeys.DATA_SOURCE, DATA_SOURCE_NAME) \\\n",
    "    .option(ConfKeys.TIME_COLUMN, \"event_date\") \\\n",
    "    .option(ConfKeys.METADATA_DB_URI, get_param(\"druid/metadata_db/uri\")) \\\n",
    "    .option(ConfKeys.METADATA_DB_USERNAME, get_param(\"druid/metadata_db/username\")) \\\n",
    "    .option(ConfKeys.METADATA_DB_PASSWORD, get_param(\"druid/metadata_db/password\")) \\\n",
    "    .option(ConfKeys.DEEP_STORAGE_S3_BUCKET, get_param(\"druid/deep_storage/bucket\")) \\\n",
    "    .option(ConfKeys.DEEP_STORAGE_S3_BASE_KEY, \"druid/segments\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "To list the written data you can run:\n",
    "\n",
    "    aws s3 --profile smoke ls --recursive \\\n",
    "      s3://{druid-deep-storage-bucket}/druid/segments/rovio_ingest_test_juho/\n",
    "\n",
    "To see something like:\n",
    "\n",
    "    2020-04-12 16:12:06        591 druid/segments/rovio_ingest_test_juho/2018-10-01T00:00:00.000Z_2018-10-02T00:00:00.000Z/2020-04-12T13:11:43.778Z/0/descriptor.json\n",
    "    2020-04-12 16:12:06       1055 druid/segments/rovio_ingest_test_juho/2018-10-01T00:00:00.000Z_2018-10-02T00:00:00.000Z/2020-04-12T13:11:43.778Z/0/index.zip\n",
    "    2020-04-12 16:12:06        591 druid/segments/rovio_ingest_test_juho/2018-10-02T00:00:00.000Z_2018-10-03T00:00:00.000Z/2020-04-12T13:11:43.778Z/0/descriptor.json\n",
    "    2020-04-12 16:12:06       1052 druid/segments/rovio_ingest_test_juho/2018-10-02T00:00:00.000Z_2018-10-03T00:00:00.000Z/2020-04-12T13:11:43.778Z/0/index.zip\n",
    "\n",
    "And run this in druid-sql (JDBC)\n",
    "\n",
    "    SELECT * FROM rovio_ingest_test_juho LIMIT 10;\n",
    "\n",
    "    __time\tapp_id\tdau\trevenue\n",
    "    2018-10-01 00:00:00\ttestclient\t5\t30\n",
    "    2018-10-02 00:00:00\ttestclient\t2\t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
